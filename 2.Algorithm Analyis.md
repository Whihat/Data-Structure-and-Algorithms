# 2.1 The RAM Model of Computation

### RAM: Random Access Machine

- Using the RAM model, we measure run time by counting up the number of steps an algorithm takes on a given instance.

- Assume given number of steps by second, this operation count converts to the actual running time.

## 2.1.1 Best, Worst, and Average-Case Complexity
![Best, worst, and average Complexity](https://github.com/Whihat/Data-Structure-and-Algorithms/blob/master/screenshot/screenshot1.jpg
)

- The worst-case is defined by MAX number of steps taken by any instance of size n. (Most useful)

- The best-case is defined by MIN number....

- The average-case is defined by Average number...


# 2.2 Big Oh Notation
![Upper and Lower bounds](https://github.com/Whihat/Data-Structure-and-Algorithms/blob/master/screenshot/screenshot2.jpg)

- For easier to talk in terms of simple upper and lower bounds of time Complexity using Big Oh notation.

The formal definition:
  - f(n) = _O(g(n))_ means c * g(n) is an upper bound on f(n)
    - f(n) ≤ c1 * g(n)
  - f(n) = _Ω(g(n))_ means c * g(n) is an lower bound on f(n)
    - f(n) ≥ c2 * g(n)
  - f(n) = _Θ(g(n))_ means c * g(n) is a exactly nice, tight bound on f(n).
    - f(n) ≤ c1 * g(n) && f(n) ≥ c2 * g(n)
  - All of above satisfy for large enough n(i.e. n ≥ n0 )

**Note**:The Big Oh notation and worst-case analysis are tools that greatly simplify out ability to compare the efficiency of Algorithms.

![Illustrating the big (a) O, (b) Ω, and (c) Θ notations
](https://github.com/Whihat/Data-Structure-and-Algorithms/blob/master/screenshot/screenshot3.jpg)

### Example
1. 3n^2 − 100n + 6 = O(n^2) = O(n^3) != O(n)
2. 3n^2 − 100n + 6 = Ω(n) = Ω(n^2) != Ω(n^3)
3. 3n^2 − 100n + 6 = Θ(n^2) != Θ(n) != Θ(n^3)
4. Problem: Is (x + y)^2 = O(x^2 + y^2)?
    - (x + y)^2 = x^2 + 2xy + y^2.
    - Calculate 2xy related to (x^2 + y^2).
    - if x ≤ y, 2xy ≤ 2y^2 ≤ 2(x^2 +y^2).
    - if x ≥ y, 2xy ≤ 2x^2 ≤ 2(x^2 +y^2).
    - Thus (x + y)^2 ≤ 3(x^2 + y^2).
    - Claim Correct!


# 2.3 Growth Rates and Dominance Relations
| n  f(n)      | lg n    | n       | nlg n   | n2      |  2^n    | n!          |
| -------------|---------|---------|---------|---------|---------|-------------|
| 10           | 0.003μs | 0.01μs  | 0.033μs | 0.1μs   | 1μs     |    | 3.63μs |
| 30           | 0.007μs | 0.03μs  | 0.147μs | 0.9μs   | 1sec    | 8.4*10^15yrs|
| 1,000        | 0.010μs | 1.00μs  | 9.966μs | 1ms     |         |             |
| 1,000,000    | 0.020μs | 1ms     | 19.93ms | 16.7 min|         |             |
| 1,000,000,000| 0.030μs | 1sec    | 29.90sec| 31.7 yrs|         |             |
                    Figure : Growth Rates of common functions

- Such enormous differences in constant factors between algorithms occur far less frequently in practice than large problems do.

## 2.3.1 Dominance Relations
- We say that a faster-growing function dominates a slower-growing one. <br />
**Note: n! ≫ 2^n ≫ n^3 ≫ n^2 ≫ nlogn ≫ n≫ logn ≫ 1**


# 2.4 Working with the Big Oh
## 2.4.1 Adding Functions
  O(f(n)) + O(g(n)) → O(max(f(n), g(n))) <br />
  Ω(f(n)) + Ω(g(n)) → Ω(max(f(n), g(n))) <br />
  Θ(f(n)) + Θ(g(n)) → Θ(max(f(n), g(n))) <br />

## 2.4.2 Multiplying Functions
  O(f(n)) * O(g(n)) → O(f(n) * g(n)) <br />
  Ω(f(n)) * Ω(g(n)) → Ω(f(n) * g(n)) <br />
  Θ(f(n)) * Θ(g(n)) → Θ(f(n) * g(n)) <br />

### Example
1. Show that Big Oh relationships are transitive. That is, if f(n) = O(g(n)) and g(n) = O(h(n)), then f(n) = O(h(n)).
   - f(n) ≤ c1g(n) ≤ c1c2h(n)
   - f(n) = O(h(n)).


# 2.5 Reasoning About Efficiency
## 2.5.1 Selection Sort
  ```
    insertion_sort(int t, int s[]){
      int i, j;    /* counters */
      int min;     /* index of minimum */

      for (i=0; i<n; i++){
        min = i;
        for (j=i+1; j<n; j++){
          if(s[j] < s[i]) min = j;
        swap(&s[min], &s[i]);
        }
      }
    }
  ```
Exact number of time: S(n) = (n−1)+(n−2)+(n−3)+...+2+1 <br />
                           ≈ (n-1)n/2 <br />
                           = O(n^2) -S(n) ≤ n(n−1) = O(n2) <br />
                           && <br />
                           = Ω(n^2) -S(n) ≥ (n/2)×(n/2) <br />
                           = Θ(n^2)

## 2.5.2 Insertion Sort
